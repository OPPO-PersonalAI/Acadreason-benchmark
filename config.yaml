# LLM evaluation configuration

models:
  gpt5mini:
    model: "gpt-5-mini"
    api_key: "${OPENAI_API_KEY}"
    base_url: "${OPENAI_API_BASE}"



experiments:
  infer_50_hints0:
    type: "infer"
    models: ["gpt5mini"]
    # models: ["gpt5mini", "gptoss"]
    prompt_file: "prompts/infer.txt"
    input_data: "data/acadreason_benchmark.jsonl"
    description: "No hints; non-math tasks"


  judge_infer_50_hints0:
    type: "judge"
    models: ["gpt5mini"]
    prompt_file: "prompts/judge.txt"
    original_bench_data: "data/acadreason_benchmark.jsonl"
    input_data: "infer_50_hints0"
    # input_field: "article"
    input_field: "response"
    return_json: true
    description: "Judge results of infer_50_hints0"



defaults:
  max_workers: 50
  request_delay: 1.0
  max_retries: 2
  timeout: 30000
  output_config:
    infer_dir: "results/infer"
    judge_dir: "results/judge"
    filename_format: "{experiment_name}_{model_name}.jsonl"
 
